<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ElevenLabs Meeting Agent</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            background: #1a1a2e;
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            overflow: hidden;
        }
        
        .avatar-container {
            position: relative;
            width: 90vmin;
            height: 90vmin;
            max-width: 800px;
            max-height: 800px;
        }
        
        .avatar {
            width: 100%;
            height: 100%;
            border-radius: 50%;
            object-fit: cover;
            border: 6px solid #4a90d9;
            box-shadow: 0 0 60px rgba(74, 144, 217, 0.3);
            transition: all 0.3s ease;
        }
        
        .avatar.speaking {
            border-color: #00d9a5;
            box-shadow: 0 0 80px rgba(0, 217, 165, 0.5);
            animation: pulse 1.5s ease-in-out infinite;
        }
        
        .avatar.listening {
            border-color: #ffd93d;
            box-shadow: 0 0 60px rgba(255, 217, 61, 0.4);
        }
        
        .hand-raised {
            position: absolute;
            top: 5%;
            right: 5%;
            font-size: 4rem;
            animation: wave 1s ease-in-out infinite;
            display: none;
        }
        
        .hand-raised.visible {
            display: block;
        }
        
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.02); }
        }
        
        @keyframes wave {
            0%, 100% { transform: rotate(0deg); }
            25% { transform: rotate(20deg); }
            75% { transform: rotate(-20deg); }
        }
        
        /* Small status indicator in corner - only shown when not connected */
        .status-indicator {
            position: fixed;
            bottom: 20px;
            left: 20px;
            padding: 8px 16px;
            border-radius: 20px;
            font-family: -apple-system, BlinkMacSystemFont, sans-serif;
            font-size: 14px;
            background: rgba(255, 255, 255, 0.1);
            color: #888;
            opacity: 0;
            transition: opacity 0.3s ease;
        }
        
        .status-indicator.visible {
            opacity: 1;
        }
        
        .status-indicator.error {
            background: rgba(255, 82, 82, 0.2);
            color: #ff5252;
        }
        
        .status-indicator.connecting {
            background: rgba(255, 217, 61, 0.2);
            color: #ffd93d;
        }
        
        /* Expression indicator at bottom of avatar circle */
        .expression-indicator {
            position: absolute;
            bottom: -15px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 1rem;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: #fff;
            background: linear-gradient(135deg, #4a90d9 0%, #357abd 100%);
            padding: 8px 20px;
            border-radius: 20px;
            z-index: 100;
            box-shadow: 0 4px 12px rgba(74, 144, 217, 0.4);
            transition: all 0.3s ease;
        }
    </style>
</head>
<body>
    <div class="avatar-container">
        <img src="/static/avatar-neutral.png" alt="Bot Avatar" class="avatar" id="avatar">
        <div class="hand-raised" id="hand-raised">âœ‹</div>
        <div class="expression-indicator" id="expression-indicator">neutral</div>
    </div>
    
    <div class="status-indicator" id="status">Connecting...</div>

    <!-- ElevenLabs JavaScript SDK -->
    <script type="module">
        // Import ElevenLabs SDK from CDN
        import { Conversation } from 'https://cdn.jsdelivr.net/npm/@11labs/client@latest/+esm';
        
        // Configuration from URL params
        const urlParams = new URLSearchParams(window.location.search);
        const agentId = urlParams.get('agent_id') || '';
        const clientId = urlParams.get('client_id') || '';
        const botName = urlParams.get('bot_name') || 'Meeting Agent';
        const agentName = urlParams.get('agent_name') || '';  // e.g., 'meeting_facilitator'
        
        // DOM elements
        const avatar = document.getElementById('avatar');
        const handRaised = document.getElementById('hand-raised');
        const statusEl = document.getElementById('status');
        const expressionIndicator = document.getElementById('expression-indicator');
        
        // Emoji mapping for expressions
        const expressionEmojis = {
            'neutral': 'ðŸ˜',
            'happy': 'ðŸ˜Š',
            'thinking': 'ðŸ¤”',
            'interested': 'ðŸ§',
            'curious': 'ðŸ¤¨',
            'encouraging': 'ðŸ‘',
            'thoughtful': 'ðŸ’­',
            'playful': 'ðŸ˜',
            'angry': 'ðŸ˜¤',
            'doubtful': 'ðŸ¤·',
            'laughing': 'ðŸ˜‚',
            'sad': 'ðŸ˜¢',
            'surprised': 'ðŸ˜®',
            'concerned': 'ðŸ˜Ÿ'
        };
        
        // Expression images - loaded dynamically based on agent config
        let expressions = ['neutral'];  // Fallback
        let idleAnimations = {};  // Track which expressions have idle GIF animations
        let speakingAnimations = {};  // Track which expressions have speaking GIF animations
        const preloadedImages = {};
        const preloadedIdleGifs = {};
        const preloadedSpeakingGifs = {};
        
        function preloadImages() {
            expressions.forEach(expr => {
                // Preload PNG (always required)
                const img = new Image();
                img.onload = () => log(`Loaded PNG: ${expr}`);
                img.onerror = () => log(`Missing PNG: ${expr} (will use neutral)`, 'warn');
                if (agentName) {
                    img.src = `/agents/${agentName}/expressions/${expr}.png`;
                } else {
                    img.src = `/static/avatar-${expr}.png`;
                }
                preloadedImages[expr] = img;
                
                // Preload idle GIF if it exists
                if (idleAnimations[expr]) {
                    const gif = new Image();
                    gif.onload = () => log(`Loaded idle GIF: ${expr}`);
                    gif.onerror = () => log(`Missing idle GIF: ${expr}`, 'warn');
                    gif.src = `/agents/${agentName}/expressions/${expr}.gif`;
                    preloadedIdleGifs[expr] = gif;
                }
                
                // Preload speaking GIF if it exists
                if (speakingAnimations[expr]) {
                    const gif = new Image();
                    gif.onload = () => log(`Loaded speaking GIF: ${expr}`);
                    gif.onerror = () => log(`Missing speaking GIF: ${expr}`, 'warn');
                    gif.src = `/agents/${agentName}/expressions/${expr}_speaking.gif`;
                    preloadedSpeakingGifs[expr] = gif;
                }
            });
        }
        
        // Fetch expressions list for this agent and preload them
        async function loadExpressions() {
            if (agentName) {
                try {
                    const response = await fetch(`/agents/${agentName}/expressions-list`);
                    if (response.ok) {
                        const data = await response.json();
                        expressions = data.expressions || expressions;
                        idleAnimations = data.idle_animations || {};
                        speakingAnimations = data.speaking_animations || {};
                        const idleCount = Object.values(idleAnimations).filter(v => v).length;
                        const speakingCount = Object.values(speakingAnimations).filter(v => v).length;
                        log(`Loaded ${expressions.length} expressions (${idleCount} idle, ${speakingCount} speaking): ${expressions.join(', ')}`);
                    }
                } catch (e) {
                    log(`Failed to load expressions: ${e.message}`, 'error');
                }
            }
            preloadImages();
        }
        
        // Set initial avatar image based on agent_name
        if (agentName) {
            avatar.src = `/agents/${agentName}/expressions/neutral.png`;
        }
        
        // Load expressions and preload images
        loadExpressions();
        
        // State
        let conversation = null;
        let serverWs = null;
        let isConnected = false;
        let isSpeaking = false;
        
        // Instrumentation state
        let conversationId = null;
        let lastModeChangeTime = null;
        let userSpeechStartTime = null;
        let botSpeechStartTime = null;
        
        // Send metric event to server
        function sendMetricEvent(eventType, data = {}) {
            if (!serverWs || serverWs.readyState !== WebSocket.OPEN) {
                log(`Cannot send metric (no server connection): ${eventType}`, 'warn');
                return;
            }
            const event = {
                type: 'metric_event',
                event_type: eventType,
                timestamp: Date.now() / 1000,  // Unix timestamp in seconds
                conversation_id: conversationId,
                client_id: clientId,
                ...data
            };
            serverWs.send(JSON.stringify(event));
            log(`ðŸ“Š Metric: ${eventType}`);
        }
        
        // Logging (console only in production mode)
        function log(message, level = 'info') {
            const timestamp = new Date().toISOString().split('T')[1].split('.')[0];
            console.log(`[${timestamp}] [${level.toUpperCase()}]`, message);
        }
        
        function showStatus(message, type = '') {
            statusEl.textContent = message;
            statusEl.className = 'status-indicator visible ' + type;
            log(`Status: ${message}`, type === 'error' ? 'error' : 'info');
            
            // Hide status after connection is established
            if (type === '' && message.includes('Connected')) {
                setTimeout(() => {
                    statusEl.classList.remove('visible');
                }, 2000);
            }
        }
        
        // Track current expression (for restoring after speaking)
        let currentExpression = 'neutral';
        let pendingExpression = 'interested'; // Expression to show after speaking ends
        
        function setExpression(expression, isAutomatic = false) {
            // Ignore legacy "speaking" expression - we now use per-expression speaking animations
            if (expression === 'speaking') {
                return;
            }
            
            // If the LLM sets an expression while we're speaking, update immediately
            // (we'll switch to the speaking variant of the new expression)
            if (!isAutomatic) {
                pendingExpression = expression;
            }
            
            // Check if we have this expression
            const hasImage = preloadedImages[expression] && preloadedImages[expression].complete && preloadedImages[expression].naturalWidth > 0;
            const hasIdleGif = preloadedIdleGifs[expression] && preloadedIdleGifs[expression].complete && preloadedIdleGifs[expression].naturalWidth > 0;
            const hasSpeakingGif = preloadedSpeakingGifs[expression] && preloadedSpeakingGifs[expression].complete && preloadedSpeakingGifs[expression].naturalWidth > 0;
            
            let usedFallback = false;
            
            // If we don't have this expression at all, fall back to neutral
            if (!hasImage && !hasIdleGif && expression !== 'neutral') {
                log(`Expression "${expression}" not found, falling back to neutral`);
                expression = 'neutral';
                usedFallback = true;
            }
            
            // Determine which asset to use based on speaking state
            let assetType = 'PNG';
            
            if (isSpeaking) {
                // When speaking, prefer speaking GIF > idle GIF > PNG
                if (speakingAnimations[expression] && (hasSpeakingGif || !usedFallback)) {
                    if (hasSpeakingGif) {
                        avatar.src = preloadedSpeakingGifs[expression].src;
                    } else {
                        avatar.src = `/agents/${agentName}/expressions/${expression}_speaking.gif`;
                    }
                    assetType = 'speaking GIF';
                } else if (idleAnimations[expression] && (hasIdleGif || !usedFallback)) {
                    // Fallback to idle GIF if no speaking GIF
                    if (hasIdleGif) {
                        avatar.src = preloadedIdleGifs[expression].src;
                    } else {
                        avatar.src = `/agents/${agentName}/expressions/${expression}.gif`;
                    }
                    assetType = 'idle GIF (no speaking)';
                } else if (hasImage) {
                    avatar.src = preloadedImages[expression].src;
                } else if (agentName) {
                    avatar.src = `/agents/${agentName}/expressions/${expression}.png`;
                }
            } else {
                // When not speaking (listening), prefer idle GIF > PNG
                if (idleAnimations[expression] && (hasIdleGif || !usedFallback)) {
                    if (hasIdleGif) {
                        avatar.src = preloadedIdleGifs[expression].src;
                    } else {
                        avatar.src = `/agents/${agentName}/expressions/${expression}.gif`;
                    }
                    assetType = 'idle GIF';
                } else if (hasImage) {
                    avatar.src = preloadedImages[expression].src;
                } else if (agentName) {
                    avatar.src = `/agents/${agentName}/expressions/${expression}.png`;
                } else {
                    // Fallback to static paths (legacy)
                    const fallbackPaths = {
                        'neutral': '/static/avatar-neutral.png',
                        'happy': '/static/avatar-happy.png',
                        'thinking': '/static/avatar-thinking.png',
                        'interested': '/static/avatar-neutral.png'
                    };
                    avatar.src = fallbackPaths[expression] || fallbackPaths['neutral'];
                }
            }
            
            currentExpression = expression;
            
            // Update the expression indicator with the expression name
            expressionIndicator.textContent = expression;
            
            log(`Expression: ${expression}${isAutomatic ? ' (auto)' : ''} (${assetType})${usedFallback ? ' (fallback)' : ''}`);
        }
        
        function setHandRaised(raised) {
            handRaised.classList.toggle('visible', raised);
        }
        
        function setSpeaking(speaking) {
            isSpeaking = speaking;
            avatar.classList.toggle('speaking', speaking);
            avatar.classList.toggle('listening', !speaking && isConnected);
            
            // Re-apply current expression with new speaking state
            // This will switch between idle and speaking variants of the same expression
            const expressionToUse = pendingExpression || currentExpression || 'neutral';
            setExpression(expressionToUse, true);
        }
        
        // Connect to our server for context updates (Recall.ai transcription)
        async function connectToServer() {
            if (!clientId) {
                log('No client_id - server context updates disabled', 'warn');
                return false;
            }
            
            try {
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const serverWsUrl = `${protocol}//${window.location.host}/ws/browser/${clientId}`;
                
                log(`Connecting to server: ${serverWsUrl}`);
                serverWs = new WebSocket(serverWsUrl);
                
                serverWs.onopen = () => {
                    log('Server WebSocket connected');
                };
                
                serverWs.onmessage = async (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        
                        if (data.type === 'start_conversation') {
                            // Server says bot is in the call
                            log('Received start signal from server - bot is in call');
                            showStatus('Starting...', 'connecting');
                            
                            // Brief delay to ensure page is fully rendered in Recall.ai
                            await new Promise(resolve => setTimeout(resolve, 1000));
                            
                            showStatus('Connecting to AI...', 'connecting');
                            await startElevenLabsConversation();
                        } else if (data.type === 'context_update' && conversation) {
                            log(`Context from ${data.speaker}: ${data.context.substring(0, 50)}...`);
                            await conversation.sendContextualUpdate(data.context);
                        } else if (data.type === 'user_message' && conversation) {
                            // Send as user input - agent will respond to this
                            log(`User message prompt: ${data.message.substring(0, 50)}...`);
                            // Try different SDK methods - one should work
                            try {
                                if (conversation.sendUserMessage) {
                                    await conversation.sendUserMessage(data.message);
                                } else if (conversation.sendUserInput) {
                                    await conversation.sendUserInput(data.message);
                                } else {
                                    // Fallback to contextual update with instruction to respond
                                    await conversation.sendContextualUpdate(`[RESPOND TO THIS] ${data.message}`);
                                }
                            } catch (e) {
                                log(`SDK method error, using contextual: ${e.message}`, 'warning');
                                await conversation.sendContextualUpdate(`[RESPOND TO THIS] ${data.message}`);
                            }
                        } else if (data.type === 'expression_change') {
                            // Handle expression change from webhook
                            log(`Expression change from server: ${data.expression}`);
                            setExpression(data.expression);
                        }
                    } catch (e) {
                        log(`Error handling server message: ${e.message}`, 'error');
                    }
                };
                
                serverWs.onerror = (error) => {
                    log(`Server WebSocket error: ${error}`, 'error');
                };
                
                serverWs.onclose = () => {
                    log('Server WebSocket closed');
                };
                
                return true;
            } catch (e) {
                log(`Failed to connect to server: ${e.message}`, 'error');
                return false;
            }
        }
        
        // Start ElevenLabs conversation
        async function startElevenLabsConversation() {
            if (conversation) {
                log('Conversation already started');
                return;
            }
            
            try {
                // Request microphone access
                log('Requesting microphone access...');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                log(`Microphone granted - tracks: ${stream.getAudioTracks().length}`);
                
                // Start ElevenLabs conversation
                log('Starting ElevenLabs conversation...');
                
                // Generate conversation ID for this session
                conversationId = `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
                
                conversation = await Conversation.startSession({
                    agentId: agentId,
                    
                    onModeChange: (mode) => {
                        log(`Mode: ${mode.mode}`);
                        const now = Date.now() / 1000;
                        
                        if (mode.mode === 'speaking') {
                            // Bot started speaking - track turn-around time
                            botSpeechStartTime = now;
                            sendMetricEvent('bot_speech_start', {
                                user_speech_end_time: lastModeChangeTime,
                                turn_around_time: lastModeChangeTime ? (now - lastModeChangeTime) * 1000 : null
                            });
                            setSpeaking(true);
                        } else if (mode.mode === 'listening') {
                            // Bot stopped speaking - ready to listen
                            // NOTE: "listening" mode does NOT mean user is speaking!
                            // We only track user speech when we get actual transcription.
                            if (isSpeaking && botSpeechStartTime) {
                                // Bot just finished speaking
                                sendMetricEvent('bot_speech_end', {
                                    duration: (now - botSpeechStartTime) * 1000
                                });
                            }
                            // Reset user speech tracking - will be set when we get transcription
                            userSpeechStartTime = null;
                            setSpeaking(false);
                        }
                        lastModeChangeTime = now;
                    },
                    
                    onStatusChange: (status) => {
                        log(`SDK Status: ${JSON.stringify(status)}`);
                        if (status.status === 'connected') {
                            isConnected = true;
                            showStatus('Connected');
                            avatar.classList.add('listening');
                            sendMetricEvent('conversation_start', { backend: 'elevenlabs_browser_sdk' });
                        } else if (status.status === 'disconnected') {
                            isConnected = false;
                            showStatus('Disconnected', 'error');
                            avatar.classList.remove('speaking', 'listening');
                            sendMetricEvent('conversation_end');
                        }
                    },
                    
                    onMessage: (message) => {
                        log(`Message: ${JSON.stringify(message).substring(0, 100)}`);
                        // Track transcription events - this is our evidence user is actually speaking
                        if (message.type === 'user_transcript' || message.source === 'user') {
                            const now = Date.now() / 1000;
                            
                            // First transcription = user started speaking
                            if (!userSpeechStartTime) {
                                userSpeechStartTime = now;
                                sendMetricEvent('user_speech_start');
                            }
                            
                            sendMetricEvent('transcription_received', {
                                transcript: message.message || message.text,
                                is_final: message.isFinal !== false
                            });
                            
                            // Final transcript = user finished speaking
                            if (message.isFinal !== false) {
                                sendMetricEvent('user_speech_end', {
                                    duration: (now - userSpeechStartTime) * 1000
                                });
                                userSpeechStartTime = null;  // Reset for next utterance
                            }
                        } else if (message.type === 'agent_response' || message.source === 'ai') {
                            sendMetricEvent('agent_response', {
                                text: message.message || message.text
                            });
                        } else if (message.type === 'interruption') {
                            sendMetricEvent('interruption');
                        }
                    },
                    
                    onError: (error) => {
                        log(`SDK Error: ${error}`, 'error');
                        showStatus('Connection error', 'error');
                        sendMetricEvent('error', { error: String(error) });
                    }
                });
                
                log('ElevenLabs conversation started!');
            } catch (error) {
                log(`Failed to start conversation: ${error.message}`, 'error');
                showStatus(`Error: ${error.message}`, 'error');
            }
        }
        
        // Main initialization
        async function init() {
            log('Initializing...');
            log(`Agent ID: ${agentId}`);
            log(`Client ID: ${clientId}`);
            log(`Agent Name: ${agentName}`);
            
            if (!agentId) {
                showStatus('Error: No agent_id provided', 'error');
                return;
            }
            
            showStatus('Waiting to join call...', 'connecting');
            
            // Connect to server first - it will tell us when to start the conversation
            await connectToServer();
        }
        
        // Auto-start when page loads
        init();
        
        // Handle page unload
        window.addEventListener('beforeunload', () => {
            if (conversation) {
                conversation.endSession();
            }
            if (serverWs) {
                serverWs.close();
            }
        });
    </script>
</body>
</html>
